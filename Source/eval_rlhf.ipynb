{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import load_peft_weights, set_peft_model_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 5.97MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 3.94MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"SeaLLMs/SeaLLMs-v3-1.5B\",\n",
    "    load_in_4bit= False,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"### Question:\\n{}\\n### Answer:\\n{}\"\n",
    "\n",
    "data= pd.concat([pd.read_json('1k9_rlhf.json').rename(columns= {'answers': 'answer'}), pd.read_json('4k_rlhf.json')])\n",
    "data= pd.DataFrame({'text': data.apply(lambda x: prompt.format(x['question'], ''), axis= 1), 'response': data.answer})\n",
    "\n",
    "data= Dataset.from_pandas(data)\n",
    "data= data.train_test_split(test_size=0.15, seed= 47)\n",
    "\n",
    "\n",
    "# train_data= data['train']\n",
    "eval_data= data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_text= np.array_split(eval_data['text'], int(len(eval_data) / 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### Question:\\nNgân sách nhà nước có phân bổ cho quỹ bảo trì đường bộ hàng năm?\\n### Answer:\\n',\n",
       " '### Question:\\nKính chắn gió của xe ô tô phải là loại kính gì?\\n### Answer:\\n',\n",
       " '### Question:\\nCông việc bảo trì đường bộ gồm những gì?\\n### Answer:\\n',\n",
       " '### Question:\\nXe mô tô có thể được kéo theo không?\\n### Answer:\\n',\n",
       " '### Question:\\nNgười điều khiển xe máy chuyên dùng có cần giấy chứng nhận kiểm định không?\\n### Answer:\\n',\n",
       " '### Question:\\nCác biện pháp bảo vệ kết cấu hạ tầng giao thông đường bộ bao gồm gì?\\n### Answer:\\n',\n",
       " '### Question:\\nGiá vé tàu có phải do Ủy ban nhân dân tỉnh quyết định?\\n### Answer:\\n',\n",
       " '### Question:\\nXe cơ giới phải có nguồn gốc như thế nào để được cấp đăng ký?\\n### Answer:\\n',\n",
       " '### Question:\\nKhông nhường đường cho xe ưu tiên bị phạt bao nhiêu?\\n### Answer:\\n',\n",
       " '### Question:\\nDự kiến từ ngày 01/7/2024, trẻ em trên 10 tuổi mới được ngồi ghế trước ô tô?\\n### Answer:\\n',\n",
       " '### Question:\\nTừ ngày 15/8/2023 các trường hợp xe nhập khẩu chưa đăng ký số máy, số khung xử lý như thế nào?\\n### Answer:\\n',\n",
       " '### Question:\\nKhi lùi xe, có cần tín hiệu báo hiệu không?\\n### Answer:\\n',\n",
       " '### Question:\\nKhi đèn tín hiệu đã tắt hoặc tiếng chuông báo hiệu ngừng, người đạp xích lô có thể làm gì?\\n### Answer:\\n',\n",
       " '### Question:\\nCó được dừng xe, đỗ xe trên các đoạn đường cong không?\\n### Answer:\\n',\n",
       " '### Question:\\nĐiều kiện về camera giám sát hành trình đối với xe ôtô?\\n### Answer:\\n',\n",
       " '### Question:\\nNgười kinh doanh vận tải hành khách có quyền từ chối vận chuyển ai trước khi xe rời bến?\\n### Answer:\\n',\n",
       " '### Question:\\nTrách nhiệm bảo đảm trật tự, an toàn giao thông đường bộ thuộc về ai?\\n### Answer:\\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_text[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_generate= {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    'top_k': 100, \n",
    "    'temperature': 0.7\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "  text= [prompt.format(i, \"\") for i in text]\n",
    "  inputs= tokenizer.batch_encode_plus(text, max_length= 256, padding= \"longest\", truncation= True, return_tensors='pt').to(\"cuda\")\n",
    "  result= tokenizer.batch_decode(model.generate(input_ids= inputs['input_ids'], attention_mask= inputs['attention_mask'], \n",
    "                                **config_generate), skip_special_tokens= True)\n",
    "\n",
    "\n",
    "  return result #[i.split('### Answer:')[1] for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    }
   ],
   "source": [
    "# inference sft \n",
    "set_peft_model_state_dict(model, load_peft_weights('sft_model'))\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "result_sft= []\n",
    "for i in eval_data: \n",
    "    result_sft.extend(chat(i.tolist()))\n",
    "\n",
    "result_sft_response= [i.split('### Answer:')[1] for i in result_sft]\n",
    "\n",
    "bleu.compute(predictions= result_sft_response, references= eval_data['response'])['bleu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# inference ppo\n",
    "set_peft_model_state_dict(model, load_peft_weights('ppo_model'))\n",
    "\n",
    "result_ppo= []\n",
    "for i in eval_data: \n",
    "    result_ppo.extend(chat(i.tolist()))\n",
    "    \n",
    "result_ppo_response= [i.split('### Answer:')[1] for i in result_ppo]\n",
    "\n",
    "bleu.compute(predictions= result_ppo_response, references= eval_data['response'])['bleu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test response score with reward \n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rm= AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "   \"reward_model\",\n",
    "    num_labels=1\n",
    ")\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\"\n",
    ")\n",
    "\n",
    "model_rm.config.update({\"pad_token_id\": rm_tokenizer.eos_token_id})\n",
    "model_rm.to(\"cuda\")\n",
    "\n",
    "reward_pipe= pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model= model_rm.merge_and_unload(),\n",
    "    tokenizer= rm_tokenizer,\n",
    "    return_token_type_ids= False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner rate of ppo model: 0.72\n"
     ]
    }
   ],
   "source": [
    "# compute_score \n",
    "like= []\n",
    "\n",
    "score_response_sft= [reward_pipe(i, **sent_kwargs) for i in result_sft]\n",
    "score_response_ppo= [reward_pipe(i, **sent_kwargs) for i in result_ppo]\n",
    "\n",
    "for i in range(len(result_ppo)):\n",
    "    if score_response_ppo[i] > score_response_sft[i]: \n",
    "        like.append(1)\n",
    "    else:\n",
    "        like.append(0)\n",
    "\n",
    "print(f'Winner rate of ppo model: {sum(like)/ len(result_ppo)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
