{"cells":[{"cell_type":"code","execution_count":null,"id":"b0d4c759-6108-4028-97a5-2a5c0f69abce","metadata":{"id":"b0d4c759-6108-4028-97a5-2a5c0f69abce"},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","\n","import pandas as pd\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, pipeline\n","\n","from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification\n","\n","from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n","\n","from peft import load_peft_weights, set_peft_model_state_dict, get_peft_model, LoraConfig\n","\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"id":"152814f6-309c-4035-8328-dbbbf94a7907","metadata":{"id":"152814f6-309c-4035-8328-dbbbf94a7907","outputId":"bcb242e0-be25-4bfb-ed1e-d57564ef282b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]}],"source":["# load reward model\n","model_rm= AutoPeftModelForSequenceClassification.from_pretrained(\n","   \"reward_model\",\n","    num_labels=1\n",")\n","rm_tokenizer = AutoTokenizer.from_pretrained(\n","    \"Qwen/Qwen2-0.5B-Instruct\"\n",")\n","\n","model_rm.config.update({\"pad_token_id\": rm_tokenizer.eos_token_id})\n","model_rm.to(\"cuda\")\n","\n","reward_pipe= pipeline(\n","    \"sentiment-analysis\",\n","    model= model_rm.merge_and_unload(),\n","    tokenizer= rm_tokenizer,\n","    return_token_type_ids= False,\n","    torch_dtype=torch.float16,\n","    device_map={\"\": 0},\n",")"]},{"cell_type":"code","execution_count":null,"id":"bb2f2531-8a56-467a-ba7d-8ed52a810e0c","metadata":{"id":"bb2f2531-8a56-467a-ba7d-8ed52a810e0c"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    # logger.warning(\"This function will be removed in the future. Please use count_params_of_model\")\n","    trainable_params = 0\n","    all_param = 0\n","    # if isinstance(model, CrossEncoder) or isinstance(model, BiEncoder):\n","    #     params= model.parameters()\n","    # else:\n","    #     params= model.model.parameters()\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"f293ae1d-7677-45a1-997d-c60375cf0967","metadata":{"id":"f293ae1d-7677-45a1-997d-c60375cf0967"},"outputs":[],"source":["import copy"]},{"cell_type":"code","execution_count":null,"id":"7fdcad67-6b51-49b8-8013-354c1097a161","metadata":{"scrolled":true,"id":"7fdcad67-6b51-49b8-8013-354c1097a161","outputId":"6c6a733b-40eb-4e02-cf38-7616d736bd51","colab":{"referenced_widgets":["a14c8593c4394c3f9b964295b9e9a6ee"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a14c8593c4394c3f9b964295b9e9a6ee","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Build model success!!!\n"]}],"source":["# load sft model\n","model_name= \"SeaLLMs/SeaLLMs-v3-1.5B\"\n","\n","model_sft= AutoModelForCausalLM.from_pretrained(model_name)\n","tokenizer_sft= AutoTokenizer.from_pretrained('sft_model', use_fast= True)\n","# tokenizer_sft.add_special_tokens({'pad_token': '<[PAD]>'})\n","# tokenizer_sft.padding= 'left'\n","\n","model_sft.config.update({'pad_token_id': tokenizer_sft.pad_token_id})\n","\n","lora_cfg = LoraConfig(\n","    r = 16,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0.,\n","    bias = \"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# load sft weight\n","model_sft= get_peft_model(model_sft, lora_cfg)\n","set_peft_model_state_dict(model_sft, load_peft_weights('sft_model'))\n","model_sft= model_sft.merge_and_unload()\n","\n","model_ref= AutoModelForCausalLMWithValueHead.from_pretrained(model_sft, torch_dtype=torch.float16) # reference model\n","model_ref.to(\"cuda\")\n","\n","# create new lora for causalLM valuehead\n","\n","model_sft= AutoModelForCausalLMWithValueHead.from_pretrained(model_sft, peft_config= lora_cfg, torch_dtype=torch.float16)\n","model_sft.to(\"cuda\")\n","\n","print('Build model success!!!')"]},{"cell_type":"code","execution_count":null,"id":"e82e1f33-ef71-4b82-a1c1-60f5d23644d9","metadata":{"id":"e82e1f33-ef71-4b82-a1c1-60f5d23644d9","outputId":"760ecc19-ba8a-4504-8a20-b722106f2479"},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 18466305 || all params: 1562180609 || trainable%: 1.182085150309275\n"]}],"source":["print_trainable_parameters(model_sft)"]},{"cell_type":"code","execution_count":null,"id":"a6b93f5b-22bb-4b2a-9a28-1d07b8a92cf3","metadata":{"id":"a6b93f5b-22bb-4b2a-9a28-1d07b8a92cf3"},"outputs":[],"source":["prompt= \"### Question:\\n{}\\n### Answer:\\n{}\"\n","\n","data= pd.concat([pd.read_json('1k9_rlhf.json').rename(columns= {'answers': 'answer'}), pd.read_json('4k_rlhf.json')])\n","\n","data= pd.DataFrame({'query': data.apply(lambda x: prompt.format(x['question'], \"\"), axis= 1)})"]},{"cell_type":"code","execution_count":null,"id":"cf905bd8-d549-4f99-b451-65dd202dd242","metadata":{"id":"cf905bd8-d549-4f99-b451-65dd202dd242"},"outputs":[],"source":["dataset= Dataset.from_pandas(data)"]},{"cell_type":"code","execution_count":null,"id":"efb5a053-1553-4c72-853c-0dd566d31773","metadata":{"id":"efb5a053-1553-4c72-853c-0dd566d31773","outputId":"51e2e81b-c1bc-48bb-d546-7bed73c0d6d1","colab":{"referenced_widgets":["270ff23b4af54a9aaf213ca871372efb"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"270ff23b4af54a9aaf213ca871372efb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5929 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize(sample):\n","    sample[\"input_ids\"] = tokenizer_sft.encode(sample[\"query\"])\n","    return sample\n","\n","dataset = dataset.map(tokenize)\n","dataset.set_format(type=\"torch\")"]},{"cell_type":"code","execution_count":null,"id":"2737d4e2-f7b5-45a6-9660-141b73c4ea36","metadata":{"id":"2737d4e2-f7b5-45a6-9660-141b73c4ea36"},"outputs":[],"source":["def collator(data):\n","  return dict((key,[d[key] for d in data]) for key in data[0])\n","\n","config = PPOConfig(\n","    learning_rate=1.41e-5,\n","    batch_size=4,\n","    mini_batch_size=1,\n","    # steps=1250,\n","    optimize_cuda_cache=True,\n","    remove_unused_columns=True,\n","    gradient_accumulation_steps= 4,\n","    # kl_penalty= 'full'\n",")\n","\n","ppo_trainer = PPOTrainer(\n","    config,\n","    model_sft,\n","    # ref_model=model_ref,\n","    tokenizer=tokenizer_sft,\n","    dataset=dataset,\n","    data_collator=collator\n",")"]},{"cell_type":"code","execution_count":null,"id":"bc107cda-f740-414d-9b69-68dbb0c52659","metadata":{"id":"bc107cda-f740-414d-9b69-68dbb0c52659"},"outputs":[],"source":["generation_kwargs = {\n","    \"min_length\": -1,\n","    \"max_new_tokens\": 200,\n","    \"top_k\": 0.0,\n","    \"top_p\": 1.0,\n","    \"do_sample\": True,\n","    \"pad_token_id\": tokenizer_sft.pad_token_id,\n","    \"eos_token_id\": tokenizer_sft.eos_token_id,\n","}\n","\n","\n","sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}"]},{"cell_type":"code","execution_count":null,"id":"73193041-e5db-4431-9997-e6b17411589b","metadata":{"scrolled":true,"id":"73193041-e5db-4431-9997-e6b17411589b","outputId":"bc58ecd3-90cb-4a4e-99b8-6c229dc4b3dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","4it [01:06, 16.19s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","5it [01:26, 17.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","6it [01:46, 18.43s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","10it [02:59, 17.79s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","11it [03:20, 18.70s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","12it [03:40, 18.97s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","13it [03:59, 19.17s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","14it [04:19, 19.33s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -6.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","15it [04:39, 19.46s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","16it [05:00, 19.96s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","17it [05:19, 19.82s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","18it [05:38, 19.45s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","19it [05:58, 19.52s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","21it [06:25, 15.88s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","25it [07:29, 14.61s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","45it [12:48, 16.31s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","46it [13:09, 17.73s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","54it [15:23, 14.96s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","55it [15:29, 12.32s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","59it [16:42, 16.57s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","61it [17:07, 14.07s/it]"]}],"source":["for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n","    query_tensors = batch[\"input_ids\"]\n","\n","    #### Get response from sft and ref\n","    response_tensors, ref_response_tensors= ppo_trainer.generate(query_tensors, generate_ref_response=True, **generation_kwargs)\n","\n","    batch[\"response\"] = [tokenizer_sft.decode(r.squeeze()) for r in response_tensors]\n","    batch[\"ref_response\"] = tokenizer_sft.batch_decode(ref_response_tensors)\n","\n","    #### Compute reward score\n","    texts = [r for  r in batch[\"response\"]]\n","    pipe_outputs = reward_pipe(texts, **sent_kwargs)\n","    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n","\n","    ref_texts = [r for r in batch[\"ref_response\"]]\n","    ref_pipe_outputs = reward_pipe(ref_texts, **sent_kwargs)\n","    ref_rewards = [torch.tensor(output[0][\"score\"]) for output in ref_pipe_outputs]\n","    batch[\"ref_rewards\"] = ref_rewards\n","\n","    #### Run PPO step\n","    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n","    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"]},{"cell_type":"code","execution_count":null,"id":"6465185c-0be9-4e80-87c2-c13968bc5a1e","metadata":{"id":"6465185c-0be9-4e80-87c2-c13968bc5a1e"},"outputs":[],"source":["texts, ref_texts"]},{"cell_type":"code","execution_count":null,"id":"e846bfe3-d777-48c2-be4b-3c0d54dc0109","metadata":{"id":"e846bfe3-d777-48c2-be4b-3c0d54dc0109"},"outputs":[],"source":["rewards, ref_rewards"]},{"cell_type":"code","execution_count":null,"id":"37db48dc-2586-4c3d-9c54-4684de3257a4","metadata":{"id":"37db48dc-2586-4c3d-9c54-4684de3257a4"},"outputs":[],"source":["inputs = tokenizer_sft(\n","[\n","    prompt.format(\n","        \"Đường cao tốc là đường gì?\", # instruction\n","        \"\"\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = ppo_trainer.model.generate(**inputs, max_new_tokens = 256, do_sample= True, top_p= 0.9, top_k= 100, temperature= 0.7)\n","tokenizer_sft.batch_decode(outputs)"]},{"cell_type":"code","execution_count":null,"id":"ddb11ada-bca0-4cc0-81bc-573c995eff44","metadata":{"id":"ddb11ada-bca0-4cc0-81bc-573c995eff44"},"outputs":[],"source":["ppo_trainer.model.save_pretrained(\"ppo_model\")"]},{"cell_type":"code","execution_count":null,"id":"06ea2a37-ab61-4d0d-9beb-8425b10a4f81","metadata":{"id":"06ea2a37-ab61-4d0d-9beb-8425b10a4f81"},"outputs":[],"source":["[tokenizer_sft.decode(r.squeeze()) for r in response_tensors]"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}